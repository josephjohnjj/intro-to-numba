{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit, int32, prange, vectorize, float64, cuda\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87680c8b",
   "metadata": {},
   "source": [
    "### Offload the computation to a GPU\n",
    "* Assumptions:\n",
    "    * N <= 512\n",
    "* Hints:\n",
    "    * Launch one block with threads <= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cf14e0",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# Hint: add the proper decorator\n",
    "def native_python_monte_kernel(array_a, array_b, array_c):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    " \n",
    "    if tx < array_a.size:  # Check array boundaries\n",
    "        array_c[tx] = 1 if array_a[tx]**2 + array_b[tx]**2 <= 1.0  else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d52e7e",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "blocks = # Number of blocks\n",
    "threads = # Number of threads\n",
    "N = 1000000\n",
    "array_a = np.random.random(N)\n",
    "array_b = np.random.random(N)\n",
    "array_c = np.zeros(N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c46482d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00078\n",
      "CPU times: user 89.7 ms, sys: 253 Î¼s, total: 90 ms\n",
      "Wall time: 130 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "native_python_monte_kernel[blocks, threads](array_a, array_b, array_c)\n",
    "print(4.0 * np.sum(array_c) / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31724393-7592-4f3e-b6a7-7e2809f163eb",
   "metadata": {},
   "source": [
    "### Try different block and thread arrangements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae52ef-92d1-43c5-b6c7-fd2e3d5b0327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
